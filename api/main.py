#!/usr/bin/env python3
"""
FastAPI ä¸»åº”ç”¨ - MLOps å¹³å° API æœåŠ¡
é›†æˆ KFServing åœ¨çº¿æ¨ç†å’Œ Feast ç‰¹å¾å­˜å‚¨
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from feast_routes import feast_router
from taxi_prediction_endpoints import taxi_router
from kafka_routes import kafka_router
from mlflow_routes import mlflow_router
from mlmd_routes import mlmd_router
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import logging
import asyncio
from datetime import datetime
import uvicorn
import pandas as pd
import numpy as np

from inference_client import KFServingInferenceClient
from feast_client import feast_client, FeatureRequest, FeatureResponse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Chicago Taxi MLOps å¹³å° API",
    description="åŸºäº KFServing å’Œ Feast çš„ Chicago Taxi è´¹ç”¨é¢„æµ‹ API æœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨ç†å®¢æˆ·ç«¯
inference_client = None

# Pydantic æ¨¡å‹å®šä¹‰
class TaxiPredictionRequest(BaseModel):
    """å‡ºç§Ÿè½¦è´¹ç”¨é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    trip_miles: float = Field(..., description="è¡Œç¨‹è·ç¦»ï¼ˆè‹±é‡Œï¼‰", example=3.5)
    trip_seconds: int = Field(..., description="è¡Œç¨‹æ—¶é•¿ï¼ˆç§’ï¼‰", example=900)
    pickup_latitude: float = Field(..., description="ä¸Šè½¦çº¬åº¦", example=41.88)
    pickup_longitude: float = Field(..., description="ä¸Šè½¦ç»åº¦", example=-87.63)
    dropoff_latitude: float = Field(..., description="ä¸‹è½¦çº¬åº¦", example=41.89)
    dropoff_longitude: float = Field(..., description="ä¸‹è½¦ç»åº¦", example=-87.62)
    pickup_hour: int = Field(..., description="ä¸Šè½¦å°æ—¶", example=14)
    pickup_day_of_week: int = Field(..., description="ä¸Šè½¦æ˜ŸæœŸå‡ ", example=1)
    pickup_month: int = Field(..., description="ä¸Šè½¦æœˆä»½", example=6)
    passenger_count: int = Field(..., description="ä¹˜å®¢æ•°é‡", example=2)
    payment_type: str = Field(..., description="æ”¯ä»˜æ–¹å¼", example="Credit Card")
    company: str = Field(..., description="å‡ºç§Ÿè½¦å…¬å¸", example="Flash Cab")
    
    class Config:
        schema_extra = {
            "example": {
                "trip_miles": 3.5,
                "trip_seconds": 900,
                "pickup_latitude": 41.88,
                "pickup_longitude": -87.63,
                "dropoff_latitude": 41.89,
                "dropoff_longitude": -87.62,
                "pickup_hour": 14,
                "pickup_day_of_week": 1,
                "pickup_month": 6,
                "passenger_count": 2,
                "payment_type": "Credit Card",
                "company": "Flash Cab"
            }
        }

class TaxiPredictionResponse(BaseModel):
    """å‡ºç§Ÿè½¦è´¹ç”¨é¢„æµ‹å“åº”æ¨¡å‹"""
    trip_id: str
    predicted_fare: float
    confidence: float
    features_used: Dict[str, Any]
    model_version: str
    timestamp: str

class BatchTaxiPredictionRequest(BaseModel):
    """æ‰¹é‡å‡ºç§Ÿè½¦è´¹ç”¨é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    trips: List[TaxiPredictionRequest] = Field(..., description="æ‰¹é‡è¡Œç¨‹è¯·æ±‚åˆ—è¡¨")
    batch_size: Optional[int] = Field(32, description="æ‰¹æ¬¡å¤§å°")
    use_feast_features: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨Feastç‰¹å¾")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str
    timestamp: str
    service_health: bool
    version: str
    feast_health: Optional[Dict[str, Any]] = None

# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global inference_client
    
    logger.info("ğŸš€ å¯åŠ¨ MLOps API æœåŠ¡...")
    
    # åˆå§‹åŒ–æ¨ç†å®¢æˆ·ç«¯
    try:
        inference_client = KFServingInferenceClient(
            service_url="http://movie-recommendation-model.default.svc.cluster.local",
            feast_repo_path="./feast/feature_repo"
        )
        logger.info("âœ… KFServing æ¨ç†å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        # ä½¿ç”¨æœ¬åœ°æµ‹è¯•åœ°å€ä½œä¸ºå¤‡ç”¨
        inference_client = KFServingInferenceClient(
            service_url="http://localhost:8080",
            feast_repo_path="./feast/feature_repo"
        )

# å…³é—­äº‹ä»¶
@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†"""
    logger.info("ğŸ›‘ å…³é—­ MLOps API æœåŠ¡...")

# API è·¯ç”±å®šä¹‰

@app.get("/", summary="API ä¿¡æ¯")
async def root():
    """
    æ ¹è·¯å¾„ - API ä¿¡æ¯
    """
    return {
        "message": "Chicago Taxi MLOps å¹³å° API æœåŠ¡",
        "version": "1.0.0",
        "description": "åŸºäº KFServing å’Œ Feast çš„ Chicago Taxi è´¹ç”¨é¢„æµ‹ API",
        "docs": "/docs",
        "health": "/health",
        "features": [
            "å‡ºç§Ÿè½¦è´¹ç”¨é¢„æµ‹",
            "Feast ç‰¹å¾å­˜å‚¨é›†æˆ",
            "æ‰¹é‡é¢„æµ‹",
            "å®æ—¶ç‰¹å¾è·å–",
            "å†å²ç‰¹å¾æŸ¥è¯¢"
        ]
    }

@app.get("/health", response_model=HealthResponse, summary="å¥åº·æ£€æŸ¥")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    try:
        # æ£€æŸ¥æ¨ç†å®¢æˆ·ç«¯çŠ¶æ€
        client_status = inference_client is not None
        
        # æ£€æŸ¥ Feast çŠ¶æ€
        feast_health = await feast_client.health_check()
        
        overall_status = "healthy"
        if not client_status or feast_health["status"] != "healthy":
            overall_status = "degraded"
        
        return HealthResponse(
            status=overall_status,
            timestamp=datetime.now().isoformat(),
            service_health=client_status,
            version="1.0.0",
            feast_health=feast_health
        )
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

@app.post("/predict", response_model=List[TaxiPredictionResponse])
async def predict(request: TaxiPredictionRequest):
    """
    å•æ¬¡æ¨ç†æ¥å£
    
    æ‰§è¡Œç”¨æˆ·-ç”µå½±æ¨èé¢„æµ‹
    """
    try:
        if not inference_client:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
        
        # éªŒè¯è¾“å…¥
        if len(request.user_ids) != len(request.movie_ids):
            raise HTTPException(
                status_code=400, 
                detail="ç”¨æˆ·IDå’Œç”µå½±IDåˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ"
            )
        
        if not request.user_ids:
            raise HTTPException(status_code=400, detail="è¾“å…¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"ğŸ¯ æ”¶åˆ°æ¨ç†è¯·æ±‚: {len(request.user_ids)} ä¸ªç”¨æˆ·-ç”µå½±å¯¹")
        
        # æ‰§è¡Œæ¨ç†
        results = inference_client.predict(request.user_ids, request.movie_ids)
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        response = [
            PredictionResponse(**result) for result in results
        ]
        
        logger.info(f"âœ… æ¨ç†å®Œæˆ: {len(response)} ä¸ªç»“æœ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨ç†æœåŠ¡é”™è¯¯: {str(e)}")

@app.post("/predict/async", response_model=List[PredictionResponse])
async def predict_async(request: PredictionRequest):
    """
    å¼‚æ­¥æ¨ç†æ¥å£
    
    æ‰§è¡Œå¼‚æ­¥ç”¨æˆ·-ç”µå½±æ¨èé¢„æµ‹
    """
    try:
        if not inference_client:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
        
        # éªŒè¯è¾“å…¥
        if len(request.user_ids) != len(request.movie_ids):
            raise HTTPException(
                status_code=400, 
                detail="ç”¨æˆ·IDå’Œç”µå½±IDåˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ"
            )
        
        logger.info(f"âš¡ æ”¶åˆ°å¼‚æ­¥æ¨ç†è¯·æ±‚: {len(request.user_ids)} ä¸ªç”¨æˆ·-ç”µå½±å¯¹")
        
        # æ‰§è¡Œå¼‚æ­¥æ¨ç†
        results = await inference_client.predict_async(request.user_ids, request.movie_ids)
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        response = [
            PredictionResponse(**result) for result in results
        ]
        
        logger.info(f"âœ… å¼‚æ­¥æ¨ç†å®Œæˆ: {len(response)} ä¸ªç»“æœ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥æ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¼‚æ­¥æ¨ç†æœåŠ¡é”™è¯¯: {str(e)}")

@app.post("/predict/batch", response_model=List[PredictionResponse])
async def predict_batch(request: BatchPredictionRequest):
    """
    æ‰¹é‡æ¨ç†æ¥å£
    
    æ‰§è¡Œæ‰¹é‡ç”¨æˆ·-ç”µå½±æ¨èé¢„æµ‹
    """
    try:
        if not inference_client:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
        
        if not request.requests:
            raise HTTPException(status_code=400, detail="æ‰¹é‡è¯·æ±‚åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"ğŸ“¦ æ”¶åˆ°æ‰¹é‡æ¨ç†è¯·æ±‚: {len(request.requests)} ä¸ªæ‰¹æ¬¡")
        
        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        results = inference_client.batch_predict(
            request.requests, 
            batch_size=request.batch_size
        )
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        response = [
            PredictionResponse(**result) for result in results
        ]
        
        logger.info(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆ: {len(response)} ä¸ªç»“æœ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ¨ç†æœåŠ¡é”™è¯¯: {str(e)}")

@app.get("/recommend/{user_id}")
async def recommend_for_user(
    user_id: int, 
    num_recommendations: int = 10,
    min_score: float = 0.5
):
    """
    ä¸ºç‰¹å®šç”¨æˆ·æ¨èç”µå½±
    
    Args:
        user_id: ç”¨æˆ·ID
        num_recommendations: æ¨èæ•°é‡
        min_score: æœ€ä½æ¨èåˆ†æ•°
    """
    try:
        if not inference_client:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
        
        # ç”Ÿæˆå€™é€‰ç”µå½±åˆ—è¡¨ (å®é™…åº”ç”¨ä¸­ä»æ•°æ®åº“è·å–)
        candidate_movies = list(range(1, num_recommendations * 2 + 1))
        user_ids = [user_id] * len(candidate_movies)
        
        logger.info(f"ğŸ¬ ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆ {num_recommendations} ä¸ªæ¨è")
        
        # æ‰§è¡Œæ¨ç†
        results = inference_client.predict(user_ids, candidate_movies)
        
        # è¿‡æ»¤å’Œæ’åº
        filtered_results = [
            result for result in results 
            if result['prediction_score'] >= min_score
        ]
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(
            filtered_results, 
            key=lambda x: x['prediction_score'], 
            reverse=True
        )[:num_recommendations]
        
        # è½¬æ¢ä¸ºæ¨èæ ¼å¼
        recommendations = [
            {
                "movie_id": result['movie_id'],
                "score": result['prediction_score'],
                "confidence": result['confidence'],
                "rank": i + 1
            }
            for i, result in enumerate(sorted_results)
        ]
        
        logger.info(f"âœ… ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆäº† {len(recommendations)} ä¸ªæ¨è")
        
        return {
            "user_id": user_id,
            "recommendations": recommendations,
            "total_candidates": len(candidate_movies),
            "filtered_count": len(filtered_results),
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ¨èç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨èæœåŠ¡é”™è¯¯: {str(e)}")

@app.get("/metrics")
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    try:
        # è¿™é‡Œå¯ä»¥é›†æˆ Prometheus æŒ‡æ ‡
        return {
            "service": "mlops-api",
            "status": "running",
            "timestamp": datetime.now().isoformat(),
            "inference_client_status": inference_client.health_check() if inference_client else False
        }
    except Exception as e:
        logger.error(f"âŒ æŒ‡æ ‡è·å–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="æŒ‡æ ‡æœåŠ¡é”™è¯¯")

# åå°ä»»åŠ¡ç¤ºä¾‹
@app.post("/predict/background")
async def predict_background(request: PredictionRequest, background_tasks: BackgroundTasks):
    """åå°æ¨ç†ä»»åŠ¡"""
    
    def run_background_prediction():
        """åå°æ‰§è¡Œæ¨ç†"""
        try:
            results = inference_client.predict(request.user_ids, request.movie_ids)
            logger.info(f"ğŸ”„ åå°æ¨ç†å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            # è¿™é‡Œå¯ä»¥å°†ç»“æœä¿å­˜åˆ°æ•°æ®åº“æˆ–å‘é€åˆ°æ¶ˆæ¯é˜Ÿåˆ—
        except Exception as e:
            logger.error(f"âŒ åå°æ¨ç†å¤±è´¥: {e}")
    
    background_tasks.add_task(run_background_prediction)
    
    return {
        "message": "åå°æ¨ç†ä»»åŠ¡å·²æäº¤",
        "request_id": f"bg_{datetime.now().timestamp()}",
        "status": "submitted"
    }

# æ³¨å†Œè·¯ç”±å™¨
app.include_router(feast_router)
app.include_router(taxi_router)
app.include_router(kafka_router)
app.include_router(mlflow_router)
app.include_router(mlmd_router)

# è¿è¡ŒæœåŠ¡
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
