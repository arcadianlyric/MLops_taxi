# Flink é›†æˆè®¾è®¡æ–¹æ¡ˆ

## ğŸ—ï¸ æ··åˆæµå¤„ç†æ¶æ„

### æ¶æ„æ¦‚è§ˆ

```mermaid
graph TD
    subgraph "æ•°æ®æº"
        A1[Taxi Raw Data]
        A2[External APIs]
        A3[Batch Data]
    end
    
    subgraph "Kafka å±‚"
        B1[taxi-raw-data]
        B2[taxi-events]
        B3[taxi-features]
    end
    
    subgraph "ç®€å•å¤„ç† (Kafka Consumer)"
        C1[å®æ—¶ç‰¹å¾æå–]
        C2[æ•°æ®éªŒè¯]
        C3[æ ¼å¼è½¬æ¢]
    end
    
    subgraph "å¤æ‚å¤„ç† (Flink)"
        D1[çª—å£èšåˆ]
        D2[çŠ¶æ€ç®¡ç†]
        D3[å¤æ‚äº‹ä»¶å¤„ç†]
        D4[ML ç‰¹å¾å·¥ç¨‹]
    end
    
    subgraph "è¾“å‡ºå±‚"
        E1[Feast ç‰¹å¾å­˜å‚¨]
        E2[å®æ—¶æ¨ç†]
        E3[ç›‘æ§å‘Šè­¦]
        E4[æ•°æ®æ¹–]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B1 --> D1
    B2 --> C2
    B2 --> D2
    B3 --> C3
    B3 --> D3
    
    C1 --> E1
    C2 --> E2
    D1 --> E1
    D2 --> E3
    D3 --> E4
    D4 --> E1
```

## ğŸ¯ åˆ†å·¥ç­–ç•¥

### Kafka Consumer å¤„ç†
- âœ… **ç®€å•ç‰¹å¾è®¡ç®—**: è·ç¦»ã€é€Ÿåº¦ã€åŸºç¡€ç»Ÿè®¡
- âœ… **æ•°æ®æ¸…æ´—**: æ ¼å¼éªŒè¯ã€å¼‚å¸¸å€¼å¤„ç†
- âœ… **å®æ—¶å“åº”**: ä½å»¶è¿Ÿåœºæ™¯ (<100ms)
- âœ… **è½»é‡çº§è½¬æ¢**: JSON æ ¼å¼è½¬æ¢ã€å­—æ®µæ˜ å°„

### Flink å¤„ç†
- ğŸš€ **å¤æ‚èšåˆ**: æ»‘åŠ¨çª—å£ã€ä¼šè¯çª—å£
- ğŸš€ **çŠ¶æ€ç®¡ç†**: ç”¨æˆ·è¡Œä¸ºçŠ¶æ€ã€åŒºåŸŸç»Ÿè®¡çŠ¶æ€
- ğŸš€ **æœºå™¨å­¦ä¹ **: åœ¨çº¿ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹æ¨ç†
- ğŸš€ **å¤æ‚äº‹ä»¶å¤„ç†**: æ¨¡å¼æ£€æµ‹ã€å¼‚å¸¸æ£€æµ‹

## ğŸ“Š å…·ä½“å®ç°æ–¹æ¡ˆ

### 1. Flink Job å®šä¹‰

```java
// TaxiStreamProcessingJob.java
public class TaxiStreamProcessingJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Kafka Source
        FlinkKafkaConsumer<TaxiEvent> source = new FlinkKafkaConsumer<>(
            "taxi-raw-data",
            new TaxiEventDeserializer(),
            kafkaProps
        );
        
        DataStream<TaxiEvent> taxiStream = env.addSource(source);
        
        // å¤æ‚çª—å£èšåˆ
        DataStream<AreaStats> areaStats = taxiStream
            .keyBy(TaxiEvent::getPickupAreaId)
            .window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(5)))
            .aggregate(new AreaStatsAggregator());
        
        // å¼‚å¸¸æ£€æµ‹
        DataStream<Alert> alerts = taxiStream
            .keyBy(TaxiEvent::getCompany)
            .process(new AnomalyDetectionFunction());
        
        // è¾“å‡ºåˆ° Kafka
        areaStats.addSink(new FlinkKafkaProducer<>("area-stats", new AreaStatsSerializer(), kafkaProps));
        alerts.addSink(new FlinkKafkaProducer<>("alerts", new AlertSerializer(), kafkaProps));
        
        env.execute("Taxi Stream Processing");
    }
}
```

### 2. Python Flink (PyFlink) ç‰ˆæœ¬

```python
# flink_taxi_processor.py
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer

def create_flink_job():
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(env)
    
    # å®šä¹‰ Kafka æºè¡¨
    t_env.execute_sql("""
        CREATE TABLE taxi_events (
            trip_id STRING,
            pickup_datetime TIMESTAMP(3),
            pickup_area_id INT,
            fare_amount DECIMAL(10,2),
            trip_distance DECIMAL(8,2),
            WATERMARK FOR pickup_datetime AS pickup_datetime - INTERVAL '5' SECOND
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'taxi-raw-data',
            'properties.bootstrap.servers' = 'localhost:9092',
            'format' = 'json'
        )
    """)
    
    # å¤æ‚çª—å£æŸ¥è¯¢
    t_env.execute_sql("""
        CREATE TABLE area_stats AS
        SELECT 
            pickup_area_id,
            TUMBLE_START(pickup_datetime, INTERVAL '5' MINUTE) as window_start,
            COUNT(*) as trip_count,
            AVG(fare_amount) as avg_fare,
            AVG(trip_distance) as avg_distance,
            STDDEV(fare_amount) as fare_stddev
        FROM taxi_events
        GROUP BY 
            pickup_area_id,
            TUMBLE(pickup_datetime, INTERVAL '5' MINUTE)
    """)
    
    # è¾“å‡ºåˆ° Kafka
    t_env.execute_sql("""
        CREATE TABLE kafka_sink (
            pickup_area_id INT,
            window_start TIMESTAMP(3),
            trip_count BIGINT,
            avg_fare DECIMAL(10,2),
            avg_distance DECIMAL(8,2),
            fare_stddev DECIMAL(10,2)
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'area-stats',
            'properties.bootstrap.servers' = 'localhost:9092',
            'format' = 'json'
        )
    """)
    
    # æ‰§è¡Œæ’å…¥
    t_env.execute_sql("INSERT INTO kafka_sink SELECT * FROM area_stats")
```

### 3. Kubernetes éƒ¨ç½²é…ç½®

```yaml
# flink-deployment.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: taxi-stream-processor
spec:
  image: flink:1.17-scala_2.12-java11
  flinkVersion: v1_17
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "4"
    state.backend: rocksdb
    state.checkpoints.dir: "s3://flink-checkpoints/taxi-processor"
    state.savepoints.dir: "s3://flink-savepoints/taxi-processor"
  serviceAccount: flink
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
  taskManager:
    resource:
      memory: "4096m"
      cpu: 2
  job:
    jarURI: local:///opt/flink/usrlib/taxi-stream-processor.jar
    parallelism: 4
    upgradeMode: savepoint
```

## ğŸ”„ è¿ç§»ç­–ç•¥

### Phase 1: å¹¶è¡Œè¿è¡Œ (2-3 å‘¨)
- [ ] éƒ¨ç½² Flink é›†ç¾¤
- [ ] å®ç°ç®€å•çš„ Flink Job (åŒºåŸŸç»Ÿè®¡)
- [ ] ä¸ç°æœ‰ Kafka å¤„ç†å™¨å¹¶è¡Œè¿è¡Œ
- [ ] å¯¹æ¯”ç»“æœéªŒè¯æ­£ç¡®æ€§

### Phase 2: é€æ­¥è¿ç§» (3-4 å‘¨)
- [ ] å°†å¤æ‚èšåˆé€»è¾‘è¿ç§»åˆ° Flink
- [ ] ä¿ç•™ Kafka Consumer å¤„ç†ç®€å•é€»è¾‘
- [ ] å®ç°çŠ¶æ€ç®¡ç†å’Œå®¹é”™
- [ ] æ€§èƒ½è°ƒä¼˜å’Œç›‘æ§

### Phase 3: ç”Ÿäº§ä¼˜åŒ– (2-3 å‘¨)
- [ ] Checkpoint å’Œ Savepoint é…ç½®
- [ ] ç›‘æ§å’Œå‘Šè­¦é›†æˆ
- [ ] è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®
- [ ] æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œ

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”é¢„æœŸ

| æŒ‡æ ‡ | å½“å‰ Kafka | å¼•å…¥ Flink å |
|------|------------|---------------|
| **ç®€å•å¤„ç†å»¶è¿Ÿ** | 50-100ms | 50-100ms |
| **å¤æ‚èšåˆå»¶è¿Ÿ** | 1-5s | 100-500ms |
| **çŠ¶æ€ç®¡ç†** | å†…å­˜é™åˆ¶ | TB çº§åˆ« |
| **å®¹é”™æ¢å¤** | æ‰‹åŠ¨é‡å¯ | è‡ªåŠ¨æ¢å¤ |
| **èµ„æºåˆ©ç”¨ç‡** | 60-70% | 80-90% |
| **å¼€å‘æ•ˆç‡** | ä¸­ç­‰ | é«˜ (SQL) |

## ğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ

### å¼•å…¥ Flink çš„æˆæœ¬
- **åŸºç¡€è®¾æ–½**: é¢å¤–çš„è®¡ç®—èµ„æº (+30-50%)
- **å­¦ä¹ æˆæœ¬**: å›¢é˜ŸåŸ¹è®­å’Œå­¦ä¹ æ›²çº¿
- **è¿ç»´å¤æ‚åº¦**: é›†ç¾¤ç®¡ç†å’Œç›‘æ§
- **å¼€å‘æ—¶é—´**: è¿ç§»å’Œæµ‹è¯•å·¥ä½œ

### å¼•å…¥ Flink çš„æ”¶ç›Š
- **å¤„ç†èƒ½åŠ›**: æ”¯æŒæ›´å¤æ‚çš„æµå¤„ç†é€»è¾‘
- **å¯æ‰©å±•æ€§**: æ›´å¥½çš„æ°´å¹³æ‰©å±•èƒ½åŠ›
- **å¼€å‘æ•ˆç‡**: Flink SQL ç®€åŒ–å¤æ‚æŸ¥è¯¢
- **å®¹é”™æ€§**: æ›´å¼ºçš„æ•…éšœæ¢å¤èƒ½åŠ›
- **æœªæ¥æ‰©å±•**: ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†åšå‡†å¤‡

## ğŸ¯ å»ºè®®

### å½“å‰é˜¶æ®µï¼šä¿æŒ Kafka-only
å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨å½“å‰æ¶æ„ï¼š
- âœ… æ•°æ®é‡ < 10GB/å¤©
- âœ… å¤„ç†é€»è¾‘ç›¸å¯¹ç®€å•
- âœ… å»¶è¿Ÿè¦æ±‚ < 1ç§’
- âœ… å›¢é˜Ÿè§„æ¨¡è¾ƒå°

### æœªæ¥å‡çº§ï¼šå¼•å…¥ Flink
å½“å‡ºç°ä»¥ä¸‹éœ€æ±‚æ—¶ï¼Œè€ƒè™‘å¼•å…¥ Flinkï¼š
- ğŸš€ éœ€è¦å¤æ‚çš„çª—å£èšåˆ (å¤šç»´åº¦ã€å¤šæ—¶é—´çª—å£)
- ğŸš€ éœ€è¦å¤§è§„æ¨¡çŠ¶æ€ç®¡ç† (ç”¨æˆ·ç”»åƒã€å®æ—¶ç‰¹å¾)
- ğŸš€ éœ€è¦æœºå™¨å­¦ä¹ åœ¨çº¿æ¨ç†
- ğŸš€ éœ€è¦å¤æ‚äº‹ä»¶å¤„ç† (CEP)
- ğŸš€ æ•°æ®é‡å¢é•¿åˆ° 100GB+/å¤©

## ğŸ”§ æ··åˆæ¶æ„æœ€ä½³å®è·µ

1. **æ˜ç¡®åˆ†å·¥**: Kafka å¤„ç†ç®€å•é€»è¾‘ï¼ŒFlink å¤„ç†å¤æ‚é€»è¾‘
2. **æ•°æ®ä¸€è‡´æ€§**: ä½¿ç”¨ç›¸åŒçš„æ—¶é—´æˆ³å’Œåˆ†åŒºç­–ç•¥
3. **ç›‘æ§ç»Ÿä¸€**: ç»Ÿä¸€çš„æŒ‡æ ‡æ”¶é›†å’Œå‘Šè­¦
4. **å®¹é”™è®¾è®¡**: ä¸¤å¥—ç³»ç»Ÿéƒ½è¦æœ‰å®¹é”™æœºåˆ¶
5. **æ€§èƒ½è°ƒä¼˜**: æ ¹æ®æ•°æ®ç‰¹å¾è°ƒæ•´å¹¶è¡Œåº¦å’Œèµ„æºé…ç½®

è¿™æ ·çš„æ··åˆæ¶æ„æ—¢ä¿æŒäº†å½“å‰ç³»ç»Ÿçš„ç®€å•æ€§ï¼Œåˆä¸ºæœªæ¥çš„æ‰©å±•ç•™ä¸‹äº†ç©ºé—´ï¼ğŸš€
