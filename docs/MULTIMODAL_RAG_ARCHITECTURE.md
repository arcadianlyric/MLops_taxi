# Multimodal Modular RAG æ¶æ„è®¾è®¡

## ğŸ¯ æ¶æ„æ¦‚è§ˆ

åŸºäºç°æœ‰ Chicago Taxi MLOps å¹³å°æ‰©å±•çš„ **Multimodal Modular RAG (Retrieval-Augmented Generation)** ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒçš„è”åˆæ£€ç´¢ä¸ç”Ÿæˆã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ç»„ä»¶

```mermaid
graph TB
    subgraph "æ•°æ®å±‚ (Data Layer)"
        A1[æ–‡æœ¬æ•°æ®é›†<br/>MS-MARCO, Natural Questions]
        A2[å›¾åƒæ•°æ®é›†<br/>COCO, Visual Genome]
        A3[å¤šæ¨¡æ€æ•°æ®é›†<br/>VQA, CLIP]
    end
    
    subgraph "åµŒå…¥å±‚ (Embedding Layer)"
        B1[æ–‡æœ¬ç¼–ç å™¨<br/>BERT, RoBERTa]
        B2[å›¾åƒç¼–ç å™¨<br/>CLIP, ResNet]
        B3[å¤šæ¨¡æ€ç¼–ç å™¨<br/>CLIP, ALIGN]
    end
    
    subgraph "å­˜å‚¨å±‚ (Storage Layer)"
        C1[å‘é‡æ•°æ®åº“<br/>Weaviate, Pinecone]
        C2[Feast ç‰¹å¾å­˜å‚¨<br/>æ–‡æœ¬/å›¾åƒç‰¹å¾]
        C3[MLflow æ¨¡å‹æ³¨å†Œ<br/>ç¼–ç å™¨æ¨¡å‹]
    end
    
    subgraph "æ£€ç´¢å±‚ (Retrieval Layer)"
        D1[æ–‡æœ¬æ£€ç´¢å™¨<br/>Dense Passage Retrieval]
        D2[å›¾åƒæ£€ç´¢å™¨<br/>Visual Similarity Search]
        D3[è·¨æ¨¡æ€æ£€ç´¢å™¨<br/>Text-Image Alignment]
    end
    
    subgraph "èåˆå±‚ (Fusion Layer)"
        E1[æ³¨æ„åŠ›æœºåˆ¶<br/>Cross-Modal Attention]
        E2[ç‰¹å¾èåˆ<br/>Early/Late Fusion]
        E3[ä¸Šä¸‹æ–‡èšåˆ<br/>Context Aggregation]
    end
    
    subgraph "ç”Ÿæˆå±‚ (Generation Layer)"
        F1[æ–‡æœ¬ç”Ÿæˆ<br/>GPT, T5]
        F2[å›¾åƒç”Ÿæˆ<br/>DALL-E, Stable Diffusion]
        F3[å¤šæ¨¡æ€ç”Ÿæˆ<br/>Flamingo, BLIP]
    end
    
    subgraph "æœåŠ¡å±‚ (Service Layer)"
        G1[FastAPI è·¯ç”±<br/>RAG Endpoints]
        G2[Streamlit UI<br/>å¤šæ¨¡æ€ç•Œé¢]
        G3[KFServing<br/>æ¨¡å‹æœåŠ¡]
    end
    
    subgraph "ç›‘æ§å±‚ (Monitoring Layer)"
        H1[æ€§èƒ½ç›‘æ§<br/>Prometheus]
        H2[è´¨é‡è¯„ä¼°<br/>BLEU, ROUGE, CLIP-Score]
        H3[æ•°æ®æ¼‚ç§»<br/>åµŒå…¥åˆ†å¸ƒç›‘æ§]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C1
    B3 --> C2
    
    C1 --> D1
    C1 --> D2
    C2 --> D3
    
    D1 --> E1
    D2 --> E2
    D3 --> E3
    
    E1 --> F1
    E2 --> F2
    E3 --> F3
    
    F1 --> G1
    F2 --> G2
    F3 --> G3
    
    G1 --> H1
    G2 --> H2
    G3 --> H3
```

## ğŸ“Š æ ‡å‡†æ•°æ®é›†é€‰æ‹©

### æ–‡æœ¬æ•°æ®é›†
- **MS-MARCO**: å¾®è½¯å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†
- **Natural Questions**: Google è‡ªç„¶é—®é¢˜æ•°æ®é›†
- **SQuAD 2.0**: æ–¯å¦ç¦é—®ç­”æ•°æ®é›†
- **FEVER**: äº‹å®éªŒè¯æ•°æ®é›†

### å›¾åƒæ•°æ®é›†
- **COCO**: é€šç”¨ç‰©ä½“è¯†åˆ«æ•°æ®é›†
- **Visual Genome**: è§†è§‰åœºæ™¯å›¾æ•°æ®é›†
- **Open Images**: Google å¼€æ”¾å›¾åƒæ•°æ®é›†
- **ImageNet**: å¤§è§„æ¨¡å›¾åƒåˆ†ç±»æ•°æ®é›†

### å¤šæ¨¡æ€æ•°æ®é›†
- **VQA 2.0**: è§†è§‰é—®ç­”æ•°æ®é›†
- **CLIP**: å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ•°æ®é›†
- **Conceptual Captions**: æ¦‚å¿µæ€§å›¾åƒæè¿°æ•°æ®é›†
- **Flickr30K**: å›¾åƒæè¿°æ•°æ®é›†

## ğŸ”§ æŠ€æœ¯æ ˆé›†æˆ

### ç°æœ‰å¹³å°é›†æˆ
```python
# åŸºäºç°æœ‰ MLOps å¹³å°çš„æ‰©å±•
â”œâ”€â”€ tfx_pipeline/              # TFX ç®¡é“æ‰©å±•
â”‚   â”œâ”€â”€ multimodal_pipeline.py    # å¤šæ¨¡æ€è®­ç»ƒç®¡é“
â”‚   â””â”€â”€ rag_components/            # RAG è‡ªå®šä¹‰ç»„ä»¶
â”œâ”€â”€ feast/                     # ç‰¹å¾å­˜å‚¨æ‰©å±•
â”‚   â”œâ”€â”€ text_features.py          # æ–‡æœ¬ç‰¹å¾å®šä¹‰
â”‚   â””â”€â”€ image_features.py         # å›¾åƒç‰¹å¾å®šä¹‰
â”œâ”€â”€ api/                       # FastAPI æ‰©å±•
â”‚   â”œâ”€â”€ rag_routes.py             # RAG API è·¯ç”±
â”‚   â””â”€â”€ multimodal_routes.py      # å¤šæ¨¡æ€ API
â”œâ”€â”€ ui/                        # Streamlit æ‰©å±•
â”‚   â”œâ”€â”€ rag_ui.py                 # RAG ç”¨æˆ·ç•Œé¢
â”‚   â””â”€â”€ multimodal_demo.py        # å¤šæ¨¡æ€æ¼”ç¤º
â””â”€â”€ models/                    # æ¨¡å‹ç»„ä»¶
    â”œâ”€â”€ encoders/                 # ç¼–ç å™¨æ¨¡å‹
    â”œâ”€â”€ retrievers/               # æ£€ç´¢å™¨
    â””â”€â”€ generators/               # ç”Ÿæˆå™¨
```

### æ ¸å¿ƒä¾èµ–
```python
# å¤šæ¨¡æ€å¤„ç†
transformers>=4.21.0
torch>=1.12.0
torchvision>=0.13.0
clip-by-openai>=1.0

# å‘é‡æ•°æ®åº“
weaviate-client>=3.15.0
pinecone-client>=2.2.0
faiss-cpu>=1.7.0

# å›¾åƒå¤„ç†
pillow>=9.0.0
opencv-python>=4.6.0
albumentations>=1.3.0

# æ–‡æœ¬å¤„ç†
spacy>=3.4.0
nltk>=3.7
sentence-transformers>=2.2.0

# RAG æ¡†æ¶
langchain>=0.0.200
llama-index>=0.7.0
```

## ğŸ¯ æ¨¡å—åŒ–è®¾è®¡

### 1. æ•°æ®æ‘„å–æ¨¡å— (Data Ingestion)
```python
class MultimodalDataIngestion:
    """å¤šæ¨¡æ€æ•°æ®æ‘„å–ç»„ä»¶"""
    
    def ingest_text_data(self, dataset_name: str):
        """æ‘„å–æ–‡æœ¬æ•°æ®é›†"""
        pass
    
    def ingest_image_data(self, dataset_name: str):
        """æ‘„å–å›¾åƒæ•°æ®é›†"""
        pass
    
    def ingest_multimodal_data(self, dataset_name: str):
        """æ‘„å–å¤šæ¨¡æ€æ•°æ®é›†"""
        pass
```

### 2. åµŒå…¥ç”Ÿæˆæ¨¡å— (Embedding Generation)
```python
class MultimodalEmbedding:
    """å¤šæ¨¡æ€åµŒå…¥ç”Ÿæˆç»„ä»¶"""
    
    def encode_text(self, text: str) -> np.ndarray:
        """æ–‡æœ¬ç¼–ç """
        pass
    
    def encode_image(self, image: PIL.Image) -> np.ndarray:
        """å›¾åƒç¼–ç """
        pass
    
    def encode_multimodal(self, text: str, image: PIL.Image) -> np.ndarray:
        """å¤šæ¨¡æ€è”åˆç¼–ç """
        pass
```

### 3. æ£€ç´¢æ¨¡å— (Retrieval)
```python
class MultimodalRetriever:
    """å¤šæ¨¡æ€æ£€ç´¢ç»„ä»¶"""
    
    def retrieve_text(self, query: str, top_k: int = 10):
        """æ–‡æœ¬æ£€ç´¢"""
        pass
    
    def retrieve_image(self, query_image: PIL.Image, top_k: int = 10):
        """å›¾åƒæ£€ç´¢"""
        pass
    
    def retrieve_cross_modal(self, text_query: str, image_query: PIL.Image):
        """è·¨æ¨¡æ€æ£€ç´¢"""
        pass
```

### 4. ç”Ÿæˆæ¨¡å— (Generation)
```python
class MultimodalGenerator:
    """å¤šæ¨¡æ€ç”Ÿæˆç»„ä»¶"""
    
    def generate_text(self, context: List[str], query: str) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    def generate_image(self, text_prompt: str) -> PIL.Image:
        """åŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒ"""
        pass
    
    def generate_multimodal_response(self, context: Dict) -> Dict:
        """ç”Ÿæˆå¤šæ¨¡æ€å“åº”"""
        pass
```

## ğŸš€ éƒ¨ç½²æ¶æ„

### Kubernetes éƒ¨ç½²æ¸…å•
```yaml
# multimodal-rag-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multimodal-rag
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multimodal-rag
  template:
    metadata:
      labels:
        app: multimodal-rag
    spec:
      containers:
      - name: rag-service
        image: multimodal-rag:latest
        ports:
        - containerPort: 8000
        env:
        - name: VECTOR_DB_URL
          value: "http://weaviate:8080"
        - name: FEAST_REPO_PATH
          value: "/app/feast"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
```

### å‘é‡æ•°æ®åº“é…ç½®
```yaml
# weaviate-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: weaviate-config
data:
  config.yaml: |
    authentication:
      anonymous_access:
        enabled: true
    authorization:
      admin_list:
        enabled: false
    query_defaults:
      limit: 20
    modules:
      text2vec-transformers:
        enabled: true
      img2vec-neural:
        enabled: true
      multi2vec-clip:
        enabled: true
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### è´¨é‡æŒ‡æ ‡
```python
class RAGQualityMetrics:
    """RAG è´¨é‡è¯„ä¼°æŒ‡æ ‡"""
    
    def calculate_retrieval_metrics(self):
        """æ£€ç´¢è´¨é‡æŒ‡æ ‡"""
        return {
            "precision_at_k": self.precision_at_k(),
            "recall_at_k": self.recall_at_k(),
            "mrr": self.mean_reciprocal_rank(),
            "ndcg": self.normalized_dcg()
        }
    
    def calculate_generation_metrics(self):
        """ç”Ÿæˆè´¨é‡æŒ‡æ ‡"""
        return {
            "bleu_score": self.bleu_score(),
            "rouge_score": self.rouge_score(),
            "clip_score": self.clip_score(),
            "semantic_similarity": self.semantic_similarity()
        }
```

### Grafana ä»ªè¡¨ç›˜é…ç½®
```json
{
  "dashboard": {
    "title": "Multimodal RAG Monitoring",
    "panels": [
      {
        "title": "Retrieval Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rag_retrieval_latency_seconds",
            "legendFormat": "Retrieval Latency"
          }
        ]
      },
      {
        "title": "Generation Quality",
        "type": "stat",
        "targets": [
          {
            "expr": "rag_bleu_score",
            "legendFormat": "BLEU Score"
          }
        ]
      }
    ]
  }
}
```

## ğŸ”„ ä¸ç°æœ‰å¹³å°é›†æˆ

### TFX Pipeline é›†æˆ
```python
# tfx_pipeline/multimodal_rag_pipeline.py
def create_multimodal_rag_pipeline():
    """åˆ›å»ºå¤šæ¨¡æ€ RAG è®­ç»ƒç®¡é“"""
    
    # æ•°æ®æ‘„å–
    text_example_gen = CsvExampleGen(input_base=text_data_root)
    image_example_gen = ImportExampleGen(input_base=image_data_root)
    
    # ç‰¹å¾å·¥ç¨‹
    text_transform = Transform(
        examples=text_example_gen.outputs['examples'],
        schema=text_schema_gen.outputs['schema'],
        module_file=text_preprocessing_module
    )
    
    image_transform = Transform(
        examples=image_example_gen.outputs['examples'],
        schema=image_schema_gen.outputs['schema'],
        module_file=image_preprocessing_module
    )
    
    # å¤šæ¨¡æ€è®­ç»ƒ
    multimodal_trainer = Trainer(
        module_file=multimodal_trainer_module,
        examples=text_transform.outputs['transformed_examples'],
        schema=text_schema_gen.outputs['schema'],
        train_args=trainer_pb2.TrainArgs(num_steps=10000),
        eval_args=trainer_pb2.EvalArgs(num_steps=1000)
    )
    
    return pipeline.Pipeline(
        pipeline_name='multimodal_rag_pipeline',
        pipeline_root=pipeline_root,
        components=[
            text_example_gen, image_example_gen,
            text_transform, image_transform,
            multimodal_trainer
        ]
    )
```

### Feast ç‰¹å¾å­˜å‚¨é›†æˆ
```python
# feast/multimodal_features.py
from feast import Entity, Feature, FeatureView, ValueType

# æ–‡æœ¬ç‰¹å¾
text_entity = Entity(name="text_id", value_type=ValueType.STRING)

text_features = FeatureView(
    name="text_embeddings",
    entities=["text_id"],
    features=[
        Feature(name="bert_embedding", dtype=ValueType.FLOAT_LIST),
        Feature(name="sentence_embedding", dtype=ValueType.FLOAT_LIST),
        Feature(name="text_length", dtype=ValueType.INT64),
    ],
    online=True,
    batch_source=text_source,
    ttl=timedelta(days=1)
)

# å›¾åƒç‰¹å¾
image_entity = Entity(name="image_id", value_type=ValueType.STRING)

image_features = FeatureView(
    name="image_embeddings",
    entities=["image_id"],
    features=[
        Feature(name="clip_embedding", dtype=ValueType.FLOAT_LIST),
        Feature(name="resnet_embedding", dtype=ValueType.FLOAT_LIST),
        Feature(name="image_width", dtype=ValueType.INT64),
        Feature(name="image_height", dtype=ValueType.INT64),
    ],
    online=True,
    batch_source=image_source,
    ttl=timedelta(days=1)
)
```

## ğŸ¨ ç”¨æˆ·ç•Œé¢è®¾è®¡

### Streamlit å¤šæ¨¡æ€ç•Œé¢
```python
# ui/multimodal_rag_ui.py
import streamlit as st
from PIL import Image

def render_multimodal_rag_interface():
    """æ¸²æŸ“å¤šæ¨¡æ€ RAG ç•Œé¢"""
    
    st.title("ğŸ¨ Multimodal RAG Assistant")
    
    # è¾“å…¥æ¨¡å¼é€‰æ‹©
    input_mode = st.selectbox(
        "é€‰æ‹©è¾“å…¥æ¨¡å¼",
        ["æ–‡æœ¬æŸ¥è¯¢", "å›¾åƒæŸ¥è¯¢", "å¤šæ¨¡æ€æŸ¥è¯¢"]
    )
    
    if input_mode == "æ–‡æœ¬æŸ¥è¯¢":
        text_query = st.text_area("è¾“å…¥æ‚¨çš„é—®é¢˜")
        if st.button("æœç´¢"):
            results = search_text(text_query)
            display_results(results)
    
    elif input_mode == "å›¾åƒæŸ¥è¯¢":
        uploaded_image = st.file_uploader("ä¸Šä¼ å›¾åƒ", type=['png', 'jpg', 'jpeg'])
        if uploaded_image and st.button("æœç´¢"):
            image = Image.open(uploaded_image)
            results = search_image(image)
            display_results(results)
    
    elif input_mode == "å¤šæ¨¡æ€æŸ¥è¯¢":
        col1, col2 = st.columns(2)
        with col1:
            text_query = st.text_area("æ–‡æœ¬æè¿°")
        with col2:
            uploaded_image = st.file_uploader("å‚è€ƒå›¾åƒ", type=['png', 'jpg', 'jpeg'])
        
        if st.button("å¤šæ¨¡æ€æœç´¢"):
            image = Image.open(uploaded_image) if uploaded_image else None
            results = search_multimodal(text_query, image)
            display_results(results)
```

## ğŸ”§ å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€æ¶æ„ (2-3 å‘¨)
- [ ] å‘é‡æ•°æ®åº“éƒ¨ç½² (Weaviate)
- [ ] åŸºç¡€ç¼–ç å™¨é›†æˆ (CLIP, BERT)
- [ ] FastAPI RAG è·¯ç”±å¼€å‘
- [ ] Streamlit åŸºç¡€ç•Œé¢

### Phase 2: æ•°æ®é›†æˆ (2-3 å‘¨)
- [ ] æ ‡å‡†æ•°æ®é›†æ‘„å–ç®¡é“
- [ ] Feast å¤šæ¨¡æ€ç‰¹å¾å®šä¹‰
- [ ] TFX Pipeline å¤šæ¨¡æ€æ‰©å±•
- [ ] æ•°æ®è´¨é‡ç›‘æ§

### Phase 3: æ£€ç´¢ä¼˜åŒ– (3-4 å‘¨)
- [ ] é«˜çº§æ£€ç´¢ç®—æ³•å®ç°
- [ ] è·¨æ¨¡æ€æ£€ç´¢ä¼˜åŒ–
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] A/B æµ‹è¯•æ¡†æ¶

### Phase 4: ç”Ÿæˆå¢å¼º (3-4 å‘¨)
- [ ] å¤§è¯­è¨€æ¨¡å‹é›†æˆ
- [ ] å›¾åƒç”Ÿæˆæ¨¡å‹é›†æˆ
- [ ] å¤šæ¨¡æ€ç”Ÿæˆä¼˜åŒ–
- [ ] è´¨é‡è¯„ä¼°ä½“ç³»

### Phase 5: ç”Ÿäº§éƒ¨ç½² (2-3 å‘¨)
- [ ] Kubernetes ç”Ÿäº§éƒ¨ç½²
- [ ] ç›‘æ§å‘Šè­¦é…ç½®
- [ ] æ€§èƒ½ä¼˜åŒ–è°ƒä¼˜
- [ ] ç”¨æˆ·æ–‡æ¡£å®Œå–„

## ğŸ“š å‚è€ƒèµ„æº

### å­¦æœ¯è®ºæ–‡
- "CLIP: Learning Transferable Visual Representations" (OpenAI, 2021)
- "Flamingo: a Visual Language Model for Few-Shot Learning" (DeepMind, 2022)
- "BLIP: Bootstrapping Language-Image Pre-training" (Salesforce, 2022)

### å¼€æºé¡¹ç›®
- [LangChain](https://github.com/hwchase17/langchain)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)
- [Weaviate](https://github.com/weaviate/weaviate)
- [CLIP](https://github.com/openai/CLIP)

è¿™ä¸ªæ¶æ„è®¾è®¡å……åˆ†åˆ©ç”¨äº†æ‚¨ç°æœ‰çš„ MLOps å¹³å°åŸºç¡€è®¾æ–½ï¼Œæä¾›äº†å®Œæ•´çš„å¤šæ¨¡æ€ RAG è§£å†³æ–¹æ¡ˆï¼ğŸš€
