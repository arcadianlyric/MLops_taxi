#!/usr/bin/env python3
"""
Kafka Kraft æµå¤„ç†ç»„ä»¶
å®ç°å®æ—¶æ•°æ®æµå¤„ç†å’Œç‰¹å¾æµå¼æ›´æ–°
"""

import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import pandas as pd
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
import asyncio
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TaxiKafkaProcessor:
    """å‡ºç§Ÿè½¦æ•°æ® Kafka æµå¤„ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– Kafka å¤„ç†å™¨
        
        Args:
            config: Kafka é…ç½®
        """
        self.config = config
        self.bootstrap_servers = config.get('bootstrap_servers', ['localhost:9092'])
        self.group_id = config.get('group_id', 'taxi-mlops-group')
        
        # Topic é…ç½®
        self.raw_data_topic = config.get('raw_data_topic', 'taxi-raw-data')
        self.features_topic = config.get('features_topic', 'taxi-features')
        self.predictions_topic = config.get('predictions_topic', 'taxi-predictions')
        self.monitoring_topic = config.get('monitoring_topic', 'taxi-monitoring')
        
        # åˆå§‹åŒ– Kafka å®¢æˆ·ç«¯
        self.producer = None
        self.consumer = None
        self.admin_client = None
        
        # å¤„ç†çŠ¶æ€
        self.is_running = False
        self.processing_thread = None
    
    def initialize(self):
        """åˆå§‹åŒ– Kafka è¿æ¥å’Œ Topics"""
        try:
            # åˆ›å»ºç®¡ç†å®¢æˆ·ç«¯
            self.admin_client = KafkaAdminClient(
                bootstrap_servers=self.bootstrap_servers,
                client_id='taxi-mlops-admin'
            )
            
            # åˆ›å»º Topics
            self._create_topics()
            
            # åˆ›å»ºç”Ÿäº§è€…
            self.producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: str(k).encode('utf-8') if k else None,
                acks='all',
                retries=3
            )
            
            # åˆ›å»ºæ¶ˆè´¹è€…
            self.consumer = KafkaConsumer(
                self.raw_data_topic,
                bootstrap_servers=self.bootstrap_servers,
                group_id=self.group_id,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                auto_offset_reset='latest',
                enable_auto_commit=True
            )
            
            logger.info("âœ… Kafka å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Kafka åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _create_topics(self):
        """åˆ›å»ºå¿…è¦çš„ Kafka Topics"""
        topics = [
            NewTopic(
                name=self.raw_data_topic,
                num_partitions=3,
                replication_factor=1,
                topic_configs={'retention.ms': '604800000'}  # 7å¤©
            ),
            NewTopic(
                name=self.features_topic,
                num_partitions=3,
                replication_factor=1,
                topic_configs={'retention.ms': '259200000'}  # 3å¤©
            ),
            NewTopic(
                name=self.predictions_topic,
                num_partitions=3,
                replication_factor=1,
                topic_configs={'retention.ms': '86400000'}   # 1å¤©
            ),
            NewTopic(
                name=self.monitoring_topic,
                num_partitions=1,
                replication_factor=1,
                topic_configs={'retention.ms': '2592000000'}  # 30å¤©
            )
        ]
        
        try:
            # è·å–ç°æœ‰ topics
            existing_topics = self.admin_client.list_topics().topics
            
            # åˆ›å»ºä¸å­˜åœ¨çš„ topics
            new_topics = [topic for topic in topics if topic.name not in existing_topics]
            
            if new_topics:
                result = self.admin_client.create_topics(new_topics)
                for topic_name, future in result.topic_futures.items():
                    try:
                        future.result()
                        logger.info(f"âœ… Topic åˆ›å»ºæˆåŠŸ: {topic_name}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Topic åˆ›å»ºå¤±è´¥ {topic_name}: {e}")
            else:
                logger.info("ğŸ“‹ æ‰€æœ‰ Topics å·²å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"âŒ Topics åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def start_processing(self):
        """å¯åŠ¨æµå¤„ç†"""
        if self.is_running:
            logger.warning("æµå¤„ç†å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._process_stream)
        self.processing_thread.daemon = True
        self.processing_thread.start()
        
        logger.info("ğŸš€ Kafka æµå¤„ç†å·²å¯åŠ¨")
    
    def stop_processing(self):
        """åœæ­¢æµå¤„ç†"""
        self.is_running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=10)
        
        logger.info("ğŸ›‘ Kafka æµå¤„ç†å·²åœæ­¢")
    
    def _process_stream(self):
        """å¤„ç†æ•°æ®æµ"""
        logger.info("ğŸ“Š å¼€å§‹å¤„ç†æ•°æ®æµ...")
        
        try:
            for message in self.consumer:
                if not self.is_running:
                    break
                
                try:
                    # å¤„ç†åŸå§‹æ•°æ®
                    raw_data = message.value
                    timestamp = datetime.now()
                    
                    logger.debug(f"æ”¶åˆ°åŸå§‹æ•°æ®: {raw_data}")
                    
                    # ç‰¹å¾æå–
                    features = self._extract_features(raw_data, timestamp)
                    
                    # å‘é€ç‰¹å¾åˆ°ç‰¹å¾ Topic
                    self._send_features(features, message.key)
                    
                    # æ¨¡å‹é¢„æµ‹ (å¦‚æœé…ç½®äº†é¢„æµ‹æœåŠ¡)
                    if self.config.get('enable_prediction', False):
                        prediction = self._make_prediction(features)
                        self._send_prediction(prediction, message.key)
                    
                    # å‘é€ç›‘æ§æŒ‡æ ‡
                    monitoring_data = self._generate_monitoring_data(
                        raw_data, features, timestamp
                    )
                    self._send_monitoring_data(monitoring_data)
                    
                except Exception as e:
                    logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"âŒ æµå¤„ç†å¼‚å¸¸: {e}")
        finally:
            logger.info("ğŸ“Š æµå¤„ç†ç»“æŸ")
    
    def _extract_features(self, raw_data: Dict[str, Any], timestamp: datetime) -> Dict[str, Any]:
        """ä»åŸå§‹æ•°æ®æå–ç‰¹å¾"""
        try:
            # åŸºäº tfx_pipeline ä¸­çš„ç‰¹å¾å®šä¹‰
            features = {
                'trip_id': raw_data.get('trip_id'),
                'trip_start_hour': timestamp.hour,
                'trip_start_day': timestamp.weekday(),
                'trip_start_month': timestamp.month,
                'trip_miles': float(raw_data.get('trip_miles', 0)),
                'fare': float(raw_data.get('fare', 0)),
                'trip_seconds': int(raw_data.get('trip_seconds', 0)),
                'pickup_latitude': float(raw_data.get('pickup_latitude', 0)),
                'pickup_longitude': float(raw_data.get('pickup_longitude', 0)),
                'dropoff_latitude': float(raw_data.get('dropoff_latitude', 0)),
                'dropoff_longitude': float(raw_data.get('dropoff_longitude', 0)),
                'pickup_census_tract': raw_data.get('pickup_census_tract', 0),
                'dropoff_census_tract': raw_data.get('dropoff_census_tract', 0),
                'pickup_community_area': raw_data.get('pickup_community_area', 0),
                'dropoff_community_area': raw_data.get('dropoff_community_area', 0),
                'company': raw_data.get('company', ''),
                'payment_type': raw_data.get('payment_type', ''),
                'event_timestamp': timestamp.isoformat(),
                'processed_at': datetime.now().isoformat()
            }
            
            # è®¡ç®—è¡ç”Ÿç‰¹å¾
            features['trip_speed'] = (
                features['trip_miles'] / (features['trip_seconds'] / 3600) 
                if features['trip_seconds'] > 0 else 0
            )
            
            features['fare_per_mile'] = (
                features['fare'] / features['trip_miles'] 
                if features['trip_miles'] > 0 else 0
            )
            
            return features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            return {}
    
    def _send_features(self, features: Dict[str, Any], key: str):
        """å‘é€ç‰¹å¾åˆ° Kafka"""
        try:
            self.producer.send(
                self.features_topic,
                key=key,
                value=features
            )
            logger.debug(f"âœ… ç‰¹å¾å·²å‘é€: {key}")
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å‘é€å¤±è´¥: {e}")
    
    def _make_prediction(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ KFServing æ¨ç†æœåŠ¡
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œè¿”å›æ¨¡æ‹Ÿé¢„æµ‹
            import random
            
            prediction = {
                'trip_id': features.get('trip_id'),
                'predicted_tips': random.choice([0, 1]),
                'prediction_score': random.uniform(0, 1),
                'model_version': 'v1.0',
                'prediction_timestamp': datetime.now().isoformat()
            }
            
            return prediction
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return {}
    
    def _send_prediction(self, prediction: Dict[str, Any], key: str):
        """å‘é€é¢„æµ‹ç»“æœåˆ° Kafka"""
        try:
            self.producer.send(
                self.predictions_topic,
                key=key,
                value=prediction
            )
            logger.debug(f"âœ… é¢„æµ‹å·²å‘é€: {key}")
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å‘é€å¤±è´¥: {e}")
    
    def _generate_monitoring_data(self, raw_data: Dict[str, Any], 
                                features: Dict[str, Any], 
                                timestamp: datetime) -> Dict[str, Any]:
        """ç”Ÿæˆç›‘æ§æ•°æ®"""
        return {
            'timestamp': timestamp.isoformat(),
            'message_count': 1,
            'processing_latency_ms': 50,  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
            'feature_count': len(features),
            'data_quality_score': 0.95,  # æ¨¡æ‹Ÿæ•°æ®è´¨é‡åˆ†æ•°
            'pipeline_stage': 'feature_extraction',
            'status': 'success'
        }
    
    def _send_monitoring_data(self, monitoring_data: Dict[str, Any]):
        """å‘é€ç›‘æ§æ•°æ®"""
        try:
            self.producer.send(
                self.monitoring_topic,
                value=monitoring_data
            )
            
        except Exception as e:
            logger.error(f"âŒ ç›‘æ§æ•°æ®å‘é€å¤±è´¥: {e}")
    
    def send_raw_data(self, data: Dict[str, Any], key: Optional[str] = None):
        """å‘é€åŸå§‹æ•°æ®åˆ° Kafka (ç”¨äºæµ‹è¯•)"""
        try:
            self.producer.send(
                self.raw_data_topic,
                key=key or str(data.get('trip_id', 'unknown')),
                value=data
            )
            self.producer.flush()
            logger.info(f"âœ… åŸå§‹æ•°æ®å·²å‘é€: {key}")
            
        except Exception as e:
            logger.error(f"âŒ åŸå§‹æ•°æ®å‘é€å¤±è´¥: {e}")
    
    def get_monitoring_stats(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è¿™é‡Œåº”è¯¥ä»ç›‘æ§ Topic è¯»å–ç»Ÿè®¡ä¿¡æ¯
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œè¿”å›æ¨¡æ‹Ÿç»Ÿè®¡
            return {
                'total_messages_processed': 1000,
                'average_processing_latency_ms': 45,
                'error_rate': 0.02,
                'throughput_per_second': 50,
                'last_updated': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç›‘æ§ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def close(self):
        """å…³é—­ Kafka è¿æ¥"""
        try:
            self.stop_processing()
            
            if self.producer:
                self.producer.close()
            
            if self.consumer:
                self.consumer.close()
            
            if self.admin_client:
                self.admin_client.close()
            
            logger.info("âœ… Kafka è¿æ¥å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­ Kafka è¿æ¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # Kafka é…ç½®
    kafka_config = {
        'bootstrap_servers': ['localhost:9092'],
        'group_id': 'taxi-mlops-group',
        'enable_prediction': True
    }
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = TaxiKafkaProcessor(kafka_config)
    
    try:
        # åˆå§‹åŒ–
        processor.initialize()
        
        # å¯åŠ¨æµå¤„ç†
        processor.start_processing()
        
        # å‘é€æµ‹è¯•æ•°æ®
        test_data = {
            'trip_id': 'test_001',
            'trip_miles': 5.2,
            'fare': 15.50,
            'trip_seconds': 1200,
            'pickup_latitude': 41.8781,
            'pickup_longitude': -87.6298,
            'dropoff_latitude': 41.8851,
            'dropoff_longitude': -87.6234,
            'pickup_community_area': 8,
            'dropoff_community_area': 32,
            'company': 'Yellow Cab',
            'payment_type': 'Credit Card'
        }
        
        processor.send_raw_data(test_data)
        
        # è¿è¡Œä¸€æ®µæ—¶é—´
        import time
        time.sleep(10)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_monitoring_stats()
        print(f"ğŸ“Š ç›‘æ§ç»Ÿè®¡: {stats}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    finally:
        processor.close()
